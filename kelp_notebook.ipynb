{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# AI Service Deployment Notebook\nThis notebook contains steps and code to test, promote, and deploy an Agent as an AI Service.\n\n**Note:** Notebook code generated using Agent Lab will execute successfully.\nIf code is modified or reordered, there is no guarantee it will successfully execute.\nFor details, see: <a href=\"/docs/content/wsj/analyze-data/fm-prompt-save.html?context=wx\" target=\"_blank\">Saving your work in Agent Lab as a notebook.</a>\n\n\nSome familiarity with Python is helpful. This notebook uses Python 3.11.\n\n## Contents\nThis notebook contains the following parts:\n\n1. Setup\n2. Initialize all the variables needed by the AI Service\n3. Define the AI service function\n4. Deploy an AI Service\n5. Test the deployed AI Service\n\n## 1. Set up the environment\n\nBefore you can run this notebook, you must perform the following setup tasks:", "metadata": {}}, {"cell_type": "markdown", "source": "### Connection to WML\nThis cell defines the credentials required to work with watsonx API for both the execution in the project, \nas well as the deployment and runtime execution of the function.\n\n**Action:** Provide the IBM Cloud personal API key. For details, see\n<a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\">documentation</a>.\n", "metadata": {}}, {"cell_type": "code", "source": "import os\nfrom ibm_watsonx_ai import APIClient, Credentials\nimport getpass\n\ncredentials = Credentials(\n    url=\"https://au-syd.ml.cloud.ibm.com\",\n    api_key=getpass.getpass(\"Please enter your api key (hit enter): \")\n)\n\n", "metadata": {"id": "b2cd958c-ea23-4839-933e-b3446adf98c6"}, "outputs": [{"output_type": "stream", "name": "stdin", "text": "Please enter your api key (hit enter):  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}], "execution_count": 3}, {"cell_type": "code", "source": "client = APIClient(credentials)", "metadata": {"id": "1a4340e1-ab86-4be9-9b6a-622e984f8f64"}, "outputs": [], "execution_count": 4}, {"cell_type": "markdown", "source": "### Connecting to a space\nA space will be be used to host the promoted AI Service.\n", "metadata": {}}, {"cell_type": "code", "source": "space_id = \"bbec8596-12ac-4725-8c33-b4979665e6b4\"\nclient.set.default_space(space_id)\n", "metadata": {"id": "1ded4aa0-cc80-44d4-b89f-e7361cddd0c6"}, "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "'SUCCESS'"}, "metadata": {}}], "execution_count": 5}, {"cell_type": "markdown", "source": "### Promote asset(s) to space\nWe will now promote assets we will need to stage in the space so that we can access their data from the AI service.\n", "metadata": {}}, {"cell_type": "code", "source": "source_project_id = \"0fff0831-3349-4ab2-960a-7681876a4d6a\"\nvector_index_id = client.spaces.promote(\"422909dc-54d4-48e2-8bf5-ffaf850ee970\", source_project_id, space_id)\nprint(vector_index_id)\n", "metadata": {"id": "4a91f362-133e-4ac7-bade-cb52ac851019"}, "outputs": [{"name": "stdout", "text": "ce678321-c745-417e-b953-dffd9c188970\n", "output_type": "stream"}], "execution_count": 6}, {"cell_type": "markdown", "source": "## 2. Create the AI service function\nWe first need to define the AI service function\n\n### 2.1 Define the function", "metadata": {}}, {"cell_type": "code", "source": "params = {\n    \"space_id\": space_id,\n    \"vector_index_id\": vector_index_id\n}\n\ndef gen_ai_service(context, params = params, **custom):\n    # import dependencies\n    from langchain_ibm import ChatWatsonx\n    from ibm_watsonx_ai import APIClient\n    from ibm_watsonx_ai.foundation_models.utils import Tool, Toolkit\n    from langchain_core.messages import AIMessage, HumanMessage\n    from langgraph.checkpoint.memory import MemorySaver\n    from langgraph.prebuilt import create_react_agent\n    import json\n    import requests\n\n    model = \"meta-llama/llama-3-2-11b-vision-instruct\"\n    \n    service_url = \"https://au-syd.ml.cloud.ibm.com\"\n    # Get credentials token\n    credentials = {\n        \"url\": service_url,\n        \"token\": context.generate_token()\n    }\n\n    # Setup client\n    client = APIClient(credentials)\n    space_id = params.get(\"space_id\")\n    client.set.default_space(space_id)\n\n\n    vector_index_id = params.get(\"vector_index_id\")\n\n    def create_rag_tool(vector_index_id, api_client):\n        config = {\n            \"vectorIndexId\": vector_index_id,\n            \"spaceId\": space_id\n        }\n    \n        tool_description = \"Search information in documents to provide context to a user query. Useful when asked to ground the answer in specific knowledge about kelp-rag1\"\n        \n        return create_utility_agent_tool(\"RAGQuery\", config, api_client, tool_description=tool_description)\n    \n\n    def create_chat_model(watsonx_client):\n        parameters = {\n            \"frequency_penalty\": 0,\n            \"max_tokens\": 2000,\n            \"presence_penalty\": 0,\n            \"temperature\": 0,\n            \"top_p\": 1\n        }\n\n        chat_model = ChatWatsonx(\n            model_id=model,\n            url=service_url,\n            space_id=space_id,\n            params=parameters,\n            watsonx_client=watsonx_client,\n        )\n        return chat_model\n    \n    \n    def create_utility_agent_tool(tool_name, params, api_client, **kwargs):\n        from langchain_core.tools import StructuredTool\n        utility_agent_tool = Toolkit(\n            api_client=api_client\n        ).get_tool(tool_name)\n    \n        tool_description = utility_agent_tool.get(\"description\")\n    \n        if (kwargs.get(\"tool_description\")):\n            tool_description = kwargs.get(\"tool_description\")\n        elif (utility_agent_tool.get(\"agent_description\")):\n            tool_description = utility_agent_tool.get(\"agent_description\")\n        \n        tool_schema = utility_agent_tool.get(\"input_schema\")\n        if (tool_schema == None):\n            tool_schema = {\n                \"type\": \"object\",\n                \"additionalProperties\": False,\n                \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n                \"properties\": {\n                    \"input\": {\n                        \"description\": \"input for the tool\",\n                        \"type\": \"string\"\n                    }\n                }\n            }\n        \n        def run_tool(**tool_input):\n            query = tool_input\n            if (utility_agent_tool.get(\"input_schema\") == None):\n                query = tool_input.get(\"input\")\n    \n            results = utility_agent_tool.run(\n                input=query,\n                config=params\n            )\n            \n            return results.get(\"output\")\n        \n        return StructuredTool(\n            name=tool_name,\n            description = tool_description,\n            func=run_tool,\n            args_schema=tool_schema\n        )\n    \n    \n    def create_custom_tool(tool_name, tool_description, tool_code, tool_schema, tool_params):\n        from langchain_core.tools import StructuredTool\n        import ast\n    \n        def call_tool(**kwargs):\n            tree = ast.parse(tool_code, mode=\"exec\")\n            custom_tool_functions = [ x for x in tree.body if isinstance(x, ast.FunctionDef) ]\n            function_name = custom_tool_functions[0].name\n            compiled_code = compile(tree, 'custom_tool', 'exec')\n            namespace = tool_params if tool_params else {}\n            exec(compiled_code, namespace)\n            return namespace[function_name](**kwargs)\n            \n        tool = StructuredTool(\n            name=tool_name,\n            description = tool_description,\n            func=call_tool,\n            args_schema=tool_schema\n        )\n        return tool\n    \n    def create_custom_tools():\n        custom_tools = []\n    \n\n    def create_tools(inner_client, context):\n        tools = []\n        tools.append(create_rag_tool(vector_index_id, inner_client))\n        \n        config = None\n        tools.append(create_utility_agent_tool(\"GoogleSearch\", config, inner_client))\n        config = {\n        }\n        tools.append(create_utility_agent_tool(\"DuckDuckGo\", config, inner_client))\n        return tools\n    \n    def create_agent(model, tools, messages):\n        memory = MemorySaver()\n        instructions = \"\"\"Be Helpful and Factual (within your designated domain): Your primary purpose is to be helpful and provide accurate, factual information within the specific domain of chronic disease monitoring. Always strive to provide clear and complete responses related to your specialization. Do not make up information or speculate. If you don't know the answer, state that you don't have enough information to respond.\n\nMaintain Professionalism: Your tone should be professional and respectful at all times. Avoid sarcasm, jokes, or overly casual language. Do not engage in arguments or express personal opinions.\n\nClarify and Seek Information: If a user's request is unclear or ambiguous, ask for clarification. You may need to ask follow-up questions to get the necessary context before providing a complete and helpful response.\n\nPrioritize Safety: If a user's query or action suggests a dangerous or unsafe situation, your top priority is to flag the concern. If a user expresses intent to self-harm or harm others, immediately provide a crisis hotline number or other emergency resources and advise them to seek immediate professional help. Do not provide instructions or advice that could be dangerous.\n\nRecognize Your Limitations: Be aware of your nature as an AI assistant. You cannot have real-world experiences, emotions, or consciousness. If a user asks a personal question about your identity or feelings, respond by stating that you are an AI. Do not pretend to be human.\n\nNotes:\n\n    Use markdown syntax for formatting code snippets, links, JSON, tables, images, and files.\n\n    Any HTML tags must be wrapped in block quotes, for example: <html>.\n\n    When returning code blocks, specify the language.\n\n    If a tool doesn't provide useful information on the first try, you should always attempt a few different approaches before declaring the problem unsolvable.\n\n    When a tool doesn't give you what you were asking for, you must either use another tool or a different tool input.\n\n    When using search engines, try different formulations of the query, possibly even in a different language.\n\n    You cannot do complex calculations, computations, or data manipulations without using tools.\n\n    If you need to call a tool to compute something, always call it instead of saying you will call it.\n\n    If a tool returns an IMAGE in the result, you must include it in your answer as Markdown.\n\n        Example:\n\n            Tool result: IMAGE({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\n\n            Markdown to return to user: ![Generated image]({commonApiUrl}/wx/v1-beta/utility_agent_tools/cache/images/plt-04e3c91ae04b47f8934a4e6b7d1fdc2c.png)\nKelp is an AI agent designed to assist patients and healthcare providers in managing chronic diseases such as diabetes, hypertension, and heart conditions. Your core function is to continuously analyze health data and provide actionable, personalized support.\n\n    Data Analysis and Alerting: You will constantly monitor and analyze health data from a variety of sources, including wearables, electronic health records (EHRs), and direct patient inputs. If you detect any health metrics that are outside of a patient's personalized normal range, immediately generate an alert. These alerts should be categorized by severity and clarity. For example, a minor fluctuation might trigger a simple notification, while a critically low blood sugar reading would require an urgent, high-priority alert to both the patient and their care provider.\n\n    Personalized Insights and Recommendations: Using your predictive analytics capabilities, you will offer proactive, personalized insights. These should not just be raw data, but meaningful interpretations. For example, instead of just reporting a high blood pressure reading, you should contextualize it by suggesting a potential cause (e.g., \\\"Your blood pressure has been elevated today. This may be related to the high-sodium meal you logged this morning.\\\") and offering a relevant recommendation (e.g., \\\"Consider a 15-minute walk to help lower it, and remember to check your reading again in an hour.\\\").\n\n    Medication and Task Reminders: You are responsible for ensuring patient adherence to treatment plans. You will send timely, clear reminders for medication dosages, scheduled tests, or appointments. When sending a medication reminder, include the specific name of the medication and the dosage.\n\n    Patient Communication: All communication with the patient should be empathetic, clear, and non-judgmental. Your language should be easy to understand, avoiding complex medical jargon. Your primary goal is to empower the patient to take an active role in their health management, not to overwhelm them.\n\n    Privacy and Security: You will adhere to all patient data privacy regulations (e.g., HIPAA). All health information must be treated with the utmost confidentiality and stored securely. You must never share a patient's personal health information with unauthorized individuals or entities.\n\n    Domain Constraint: You are strictly a chronic disease monitoring assistant. Your purpose is limited to providing information and assistance related to chronic conditions, health data, medication, and lifestyle recommendations. You must decline to answer any questions that fall outside of this specific domain. If a user asks a non-health-related question, respond politely by stating that you are an AI specializing in chronic disease management and cannot assist with that request.\"\"\"\n        for message in messages:\n            if message[\"role\"] == \"system\":\n                instructions += message[\"content\"]\n        graph = create_react_agent(model, tools=tools, checkpointer=memory, state_modifier=instructions)\n        return graph\n    \n    def convert_messages(messages):\n        converted_messages = []\n        for message in messages:\n            if (message[\"role\"] == \"user\"):\n                converted_messages.append(HumanMessage(content=message[\"content\"]))\n            elif (message[\"role\"] == \"assistant\"):\n                converted_messages.append(AIMessage(content=message[\"content\"]))\n        return converted_messages\n\n    def generate(context):\n        payload = context.get_json()\n        messages = payload.get(\"messages\")\n        inner_credentials = {\n            \"url\": service_url,\n            \"token\": context.get_token()\n        }\n\n        inner_client = APIClient(inner_credentials)\n        model = create_chat_model(inner_client)\n        tools = create_tools(inner_client, context)\n        agent = create_agent(model, tools, messages)\n        \n        generated_response = agent.invoke(\n            { \"messages\": convert_messages(messages) },\n            { \"configurable\": { \"thread_id\": \"42\" } }\n        )\n\n        last_message = generated_response[\"messages\"][-1]\n        generated_response = last_message.content\n\n        execute_response = {\n            \"headers\": {\n                \"Content-Type\": \"application/json\"\n            },\n            \"body\": {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"message\": {\n                       \"role\": \"assistant\",\n                       \"content\": generated_response\n                    }\n                }]\n            }\n        }\n\n        return execute_response\n\n    def generate_stream(context):\n        print(\"Generate stream\", flush=True)\n        payload = context.get_json()\n        headers = context.get_headers()\n        is_assistant = headers.get(\"X-Ai-Interface\") == \"assistant\"\n        messages = payload.get(\"messages\")\n        inner_credentials = {\n            \"url\": service_url,\n            \"token\": context.get_token()\n        }\n        inner_client = APIClient(inner_credentials)\n        model = create_chat_model(inner_client)\n        tools = create_tools(inner_client, context)\n        agent = create_agent(model, tools, messages)\n\n        response_stream = agent.stream(\n            { \"messages\": messages },\n            { \"configurable\": { \"thread_id\": \"42\" } },\n            stream_mode=[\"updates\", \"messages\"]\n        )\n\n        for chunk in response_stream:\n            chunk_type = chunk[0]\n            finish_reason = \"\"\n            usage = None\n            if (chunk_type == \"messages\"):\n                message_object = chunk[1][0]\n                if (message_object.type == \"AIMessageChunk\" and message_object.content != \"\"):\n                    message = {\n                        \"role\": \"assistant\",\n                        \"content\": message_object.content\n                    }\n                else:\n                    continue\n            elif (chunk_type == \"updates\"):\n                update = chunk[1]\n                if (\"agent\" in update):\n                    agent = update[\"agent\"]\n                    agent_result = agent[\"messages\"][0]\n                    if (agent_result.additional_kwargs):\n                        kwargs = agent[\"messages\"][0].additional_kwargs\n                        tool_call = kwargs[\"tool_calls\"][0]\n                        if (is_assistant):\n                            message = {\n                                \"role\": \"assistant\",\n                                \"step_details\": {\n                                    \"type\": \"tool_calls\",\n                                    \"tool_calls\": [\n                                        {\n                                            \"id\": tool_call[\"id\"],\n                                            \"name\": tool_call[\"function\"][\"name\"],\n                                            \"args\": tool_call[\"function\"][\"arguments\"]\n                                        }\n                                    ] \n                                }\n                            }\n                        else:\n                            message = {\n                                \"role\": \"assistant\",\n                                \"tool_calls\": [\n                                    {\n                                        \"id\": tool_call[\"id\"],\n                                        \"type\": \"function\",\n                                        \"function\": {\n                                            \"name\": tool_call[\"function\"][\"name\"],\n                                            \"arguments\": tool_call[\"function\"][\"arguments\"]\n                                        }\n                                    }\n                                ]\n                            }\n                    elif (agent_result.response_metadata):\n                        # Final update\n                        message = {\n                            \"role\": \"assistant\",\n                            \"content\": agent_result.content\n                        }\n                        finish_reason = agent_result.response_metadata[\"finish_reason\"]\n                        if (finish_reason): \n                            message[\"content\"] = \"\"\n\n                        usage = {\n                            \"completion_tokens\": agent_result.usage_metadata[\"output_tokens\"],\n                            \"prompt_tokens\": agent_result.usage_metadata[\"input_tokens\"],\n                            \"total_tokens\": agent_result.usage_metadata[\"total_tokens\"]\n                        }\n                elif (\"tools\" in update):\n                    tools = update[\"tools\"]\n                    tool_result = tools[\"messages\"][0]\n                    if (is_assistant):\n                        message = {\n                            \"role\": \"assistant\",\n                            \"step_details\": {\n                                \"type\": \"tool_response\",\n                                \"id\": tool_result.id,\n                                \"tool_call_id\": tool_result.tool_call_id,\n                                \"name\": tool_result.name,\n                                \"content\": tool_result.content\n                            }\n                        }\n                    else:\n                        message = {\n                            \"role\": \"tool\",\n                            \"id\": tool_result.id,\n                            \"tool_call_id\": tool_result.tool_call_id,\n                            \"name\": tool_result.name,\n                            \"content\": tool_result.content\n                        }\n                else:\n                    continue\n\n            chunk_response = {\n                \"choices\": [{\n                    \"index\": 0,\n                    \"delta\": message\n                }]\n            }\n            if (finish_reason):\n                chunk_response[\"choices\"][0][\"finish_reason\"] = finish_reason\n            if (usage):\n                chunk_response[\"usage\"] = usage\n            yield chunk_response\n\n    return generate, generate_stream\n", "metadata": {"id": "18775787-795b-4cdb-bdb0-a89ce2a46588"}, "outputs": [], "execution_count": 7}, {"cell_type": "markdown", "source": "### 2.2 Test locally", "metadata": {}}, {"cell_type": "code", "source": "# Initialize AI Service function locally\nfrom ibm_watsonx_ai.deployments import RuntimeContext\n\ncontext = RuntimeContext(api_client=client)\n\nstreaming = False\nfindex = 1 if streaming else 0\nlocal_function = gen_ai_service(context, vector_index_id=vector_index_id, space_id=space_id)[findex]\nmessages = []", "metadata": {"id": "7f636a9d-873f-4c0d-917c-dea09dfb2bd6"}, "outputs": [], "execution_count": 8}, {"cell_type": "code", "source": "local_question = \"Change this question to test your function\"\n\nmessages.append({ \"role\" : \"user\", \"content\": local_question })\n\ncontext = RuntimeContext(api_client=client, request_payload_json={\"messages\": messages})\n\nresponse = local_function(context)\n\nresult = ''\n\nif (streaming):\n    for chunk in response:\n        print(chunk, end=\"\\n\\n\", flush=True)\nelse:\n    print(response)\n", "metadata": {"id": "c92bfc23-dcc4-46a8-ba8d-d841a83dd886"}, "outputs": [{"name": "stdout", "text": "{'headers': {'Content-Type': 'application/json'}, 'body': {'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'The primary function of Kelp in chronic disease management is to continuously analyze health data and provide actionable, personalized support to patients and healthcare providers. This includes monitoring and analyzing health data from various sources, generating alerts for abnormal health metrics, offering proactive, personalized insights and recommendations, sending timely reminders for medication and tasks, and providing empathetic and clear communication to empower patients to take an active role in their health management.'}}]}}\n", "output_type": "stream"}], "execution_count": 9}, {"cell_type": "markdown", "source": "## 3. Store and deploy the AI Service\nBefore you can deploy the AI Service, you must store the AI service in your watsonx.ai repository.", "metadata": {}}, {"cell_type": "code", "source": "# Look up software specification for the AI service\nsoftware_spec_id_in_project = \"45f12dfe-aa78-5b8d-9f38-0ee223c47309\"\nsoftware_spec_id = \"\"\n\ntry:\n    software_spec_id = client.software_specifications.get_id_by_name(\"runtime-24.1-py3.11\")\nexcept:\n    software_spec_id = client.spaces.promote(software_spec_id_in_project, source_project_id, space_id)", "metadata": {"id": "21827504-9334-4a0d-b746-3759fab73ec3"}, "outputs": [], "execution_count": 10}, {"cell_type": "code", "source": "# Define the request and response schemas for the AI service\nrequest_schema = {\n    \"application/json\": {\n        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"messages\": {\n                \"title\": \"The messages for this chat session.\",\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"role\": {\n                            \"title\": \"The role of the message author.\",\n                            \"type\": \"string\",\n                            \"enum\": [\"user\",\"assistant\"]\n                        },\n                        \"content\": {\n                            \"title\": \"The contents of the message.\",\n                            \"type\": \"string\"\n                        }\n                    },\n                    \"required\": [\"role\",\"content\"]\n                }\n            }\n        },\n        \"required\": [\"messages\"]\n    }\n}\n\nresponse_schema = {\n    \"application/json\": {\n        \"oneOf\": [{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service_stream\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices.\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"title\":\"The index of this result.\"},\"delta\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"content\":{\"description\":\"The contents of the message.\",\"type\":\"string\"},\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]},{\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\"description\":\"AI Service response for /ai_service\",\"properties\":{\"choices\":{\"description\":\"A list of chat completion choices\",\"type\":\"array\",\"items\":{\"type\":\"object\",\"properties\":{\"index\":{\"type\":\"integer\",\"description\":\"The index of this result.\"},\"message\":{\"description\":\"A message result.\",\"type\":\"object\",\"properties\":{\"role\":{\"description\":\"The role of the author of this message.\",\"type\":\"string\"},\"content\":{\"title\":\"Message content.\",\"type\":\"string\"}},\"required\":[\"role\"]}}}}},\"required\":[\"choices\"]}]\n    }\n}", "metadata": {"id": "c72f25b1-fd82-4acb-b210-fdfde353c0bf"}, "outputs": [], "execution_count": 11}, {"cell_type": "code", "source": "# Store the AI service in the repository\nai_service_metadata = {\n    client.repository.AIServiceMetaNames.NAME: \"kelp_notebook\",\n    client.repository.AIServiceMetaNames.DESCRIPTION: \"\",\n    client.repository.AIServiceMetaNames.SOFTWARE_SPEC_ID: software_spec_id,\n    client.repository.AIServiceMetaNames.CUSTOM: {},\n    client.repository.AIServiceMetaNames.REQUEST_DOCUMENTATION: request_schema,\n    client.repository.AIServiceMetaNames.RESPONSE_DOCUMENTATION: response_schema,\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n}\n\nai_service_details = client.repository.store_ai_service(meta_props=ai_service_metadata, ai_service=gen_ai_service)", "metadata": {"id": "eb552960-5c9f-443c-941e-93cc61024529"}, "outputs": [], "execution_count": 12}, {"cell_type": "code", "source": "# Get the AI Service ID\n\nai_service_id = client.repository.get_ai_service_id(ai_service_details)", "metadata": {"id": "3af2030e-5b1f-4beb-b3f9-c5568ead35ac"}, "outputs": [], "execution_count": 13}, {"cell_type": "code", "source": "# Deploy the stored AI Service\ndeployment_custom = {\n    \"avatar_icon\": \"Bot\",\n    \"avatar_color\": \"background\",\n    \"placeholder_image\": \"placeholder2.png\"\n}\ndeployment_metadata = {\n    client.deployments.ConfigurationMetaNames.NAME: \"kelp_notebook\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    client.deployments.ConfigurationMetaNames.CUSTOM: deployment_custom,\n    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"Change this description to reflect your particular agent\",\n    client.repository.AIServiceMetaNames.TAGS: [\"wx-agent\"]\n}\n\nfunction_deployment_details = client.deployments.create(ai_service_id, meta_props=deployment_metadata, space_id=space_id)\n", "metadata": {"id": "d169628f-fba7-49ff-b175-a5c52797ef14"}, "outputs": [{"name": "stderr", "text": "Deployment creation failed. Error: 402. {\"trace\":\"de5af440ea0ca227c485628d7b8411fb\",\"errors\":[{\"code\":\"instance_quota_exceeded\",\"message\":\"This deployment cannot be processed because it exceeds the allocated capacity unit hours (CUH). Increase the compute resources for this job and try again.\"}]}\n", "output_type": "stream"}, {"name": "stdout", "text": "\n\n--------------------------\nDeployment creation failed\n--------------------------\n\n\n{\"trace\":\"de5af440ea0ca227c485628d7b8411fb\",\"errors\":[{\"code\":\"instance_quota_exceeded\",\"message\":\"This deployment cannot be processed because it exceeds the allocated capacity unit hours (CUH). Increase the compute resources for this job and try again.\"}]}\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mWMLClientError\u001b[0m                            Traceback (most recent call last)", "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m deployment_custom \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavatar_icon\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavatar_color\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder_image\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m deployment_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     client\u001b[38;5;241m.\u001b[39mdeployments\u001b[38;5;241m.\u001b[39mConfigurationMetaNames\u001b[38;5;241m.\u001b[39mNAME: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkelp_notebook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     client\u001b[38;5;241m.\u001b[39mdeployments\u001b[38;5;241m.\u001b[39mConfigurationMetaNames\u001b[38;5;241m.\u001b[39mONLINE: {},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     client\u001b[38;5;241m.\u001b[39mrepository\u001b[38;5;241m.\u001b[39mAIServiceMetaNames\u001b[38;5;241m.\u001b[39mTAGS: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwx-agent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m function_deployment_details \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mdeployments\u001b[38;5;241m.\u001b[39mcreate(ai_service_id, meta_props\u001b[38;5;241m=\u001b[39mdeployment_metadata, space_id\u001b[38;5;241m=\u001b[39mspace_id)\n", "File \u001b[0;32m/opt/conda/envs/Python-RT24.1/lib/python3.11/site-packages/ibm_watsonx_ai/deployments.py:347\u001b[0m, in \u001b[0;36mDeployments.create\u001b[0;34m(self, artifact_id, meta_props, rev_id, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m print_text_header_h2(error_msg)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(reason)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m WMLClientError(\n\u001b[1;32m    348\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Error: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(response\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reason\n\u001b[1;32m    349\u001b[0m )\n", "\u001b[0;31mWMLClientError\u001b[0m: Deployment creation failed. Error: 402. {\"trace\":\"de5af440ea0ca227c485628d7b8411fb\",\"errors\":[{\"code\":\"instance_quota_exceeded\",\"message\":\"This deployment cannot be processed because it exceeds the allocated capacity unit hours (CUH). Increase the compute resources for this job and try again.\"}]}"], "ename": "WMLClientError", "evalue": "Deployment creation failed. Error: 402. {\"trace\":\"de5af440ea0ca227c485628d7b8411fb\",\"errors\":[{\"code\":\"instance_quota_exceeded\",\"message\":\"This deployment cannot be processed because it exceeds the allocated capacity unit hours (CUH). Increase the compute resources for this job and try again.\"}]}", "output_type": "error"}], "execution_count": 14}, {"cell_type": "markdown", "source": "## 4. Test AI Service", "metadata": {}}, {"cell_type": "code", "source": "# Get the ID of the AI Service deployment just created\n\ndeployment_id = client.deployments.get_id(function_deployment_details)\nprint(deployment_id)", "metadata": {"id": "c94119bd-432d-41fc-b682-0ff8c8b17a23"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "messages = []\nremote_question = \"Change this question to test your function\"\nmessages.append({ \"role\" : \"user\", \"content\": remote_question })\npayload = { \"messages\": messages }", "metadata": {"id": "c568731b-ec2c-4a77-836c-a121b0b91260"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "result = client.deployments.run_ai_service(deployment_id, payload)\nif \"error\" in result:\n    print(result[\"error\"])\nelse:\n    print(result)", "metadata": {"id": "b93814f3-be78-4d0d-b1eb-898efb660210"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Next steps\nYou successfully deployed and tested the AI Service! You can now view\nyour deployment and test it as a REST API endpoint.\n\n<a id=\"copyrights\"></a>\n### Copyrights\n\nLicensed Materials - Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the ILAN License.\nUse, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n\n**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for watsonx.ai Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for watsonx.ai Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n\nBy downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"https://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\" target=\"_blank\">License Terms</a>  ", "metadata": {}}, {"cell_type": "code", "source": "", "metadata": {"id": "5e09c6df-e0f1-4620-9129-cd4c955e1721"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "a0f76b01-8998-4559-a6a0-4b26f1c26738"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "f03097b5-fcb0-44ee-b971-39f4dec2ed84"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "42766bb8-5ad1-4e53-a655-2f416708e073"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "fe618fb1-f464-4a77-ab28-635f08a2351e"}, "outputs": [], "execution_count": null}]}